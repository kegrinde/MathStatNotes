[["index.html", "Mathematical Statistics Welcome!", " Mathematical Statistics Kelsey Grinde Updated: 2021-02-04 Welcome! Welcome to the course notes for MATH/STAT 455: Mathematical Statistics at Macalester College. These notes were created by Kelsey Grinde and draw heavily from the course textbook: An Introduction to Mathematical Statistics and Its Applications by Richard Larsen and Morris Marx (6th Edition) I will be editing and adding to these notes throughout Spring 2021, so please check back for updates! If you find any typos or have other questions, please email kgrinde@macalester.edu. "],["probability-review.html", "1 Probability Review 1.1 Learning Goals 1.2 (Optional) Textbook Reading Guide", " 1 Probability Review This course builds on topics that you covered in MATH/STAT 354: Probability. Some of you might have taken Probability last semester, and for others it might have been awhile. I certainly don’t expect you to remember everything you learned in that class, but there are some concepts from Probability that we will use fairly often in this class (see the list below): if you’re rusty on any of these, I recommend spending some time reviewing. 1.1 Learning Goals Distinguish between important probability models (e.g., Normal, Binomial) Derive the expected value and variance of a single random variable or a sum of random variables Find the distribution of order statistics (e.g., minimum, maximum) Define the moment generating function and use it to find moments or identify pdfs 1.2 (Optional) Textbook Reading Guide Read: Chapters 2–4 (pages 15–277) Definitions: probability density function (discrete, continuous) cumulative distribution function (discrete, continuous) joint probability density function conditional probability density function independence random variable expected value variance \\(r^{th}\\) moment covariance random sample order statistic moment generating function Binomial distribution Poisson distribution Geometric distribution Negative Binomial distribution Normal/Gaussian distribution Gamma distribution Exponential distribution Uniform distribution Theorems: Law of Total Probability – Theorem 2.4.1 Bayes’ Theorem – Theorem 2.4.2 Relationship between pdf and cdf – Theorem 3.4.1 and Theorem 3.7.3 Expected value and variance of linear transformations of random variables – Corollary 3.5.1, Theorem 3.6.2, Theorem 3.9.2, Theorem 3.9.5 Relationship between mean and variance – Theorem 3.6.1 Finding a marginal pdf from a joint pdf – Theorem 3.7.1 and Theorem 3.7.2 Independence of random variables and joint pdfs – Theorem 3.7.4 Expected value of a product of independent random variables – Theorem 3.9.3 Covariance of independent random variables – Theorem 3.9.4 Finding the pdf and cdf of an order statistic – Theorem 3.10.1 Using MGFs to find moments – Theorem 3.12.1 Using MGFs to identify pdfs – Theorem 3.12.2 and Theorem 3.12.3 Central Limit Theorem – Theorem 4.3.2 Note: page and example numbers correspond to the 6th Edition of Larsen &amp; Marx and may not correspond directly to earlier editions of the textbook. "],["estimation.html", "2 Estimation 2.1 Learning Goals 2.2 Least Squares Estimation 2.3 Maximum Likelihood Estimation 2.4 The Method of Moments", " 2 Estimation In Probability, you calculated probabilities of events by assuming a probability model for data and then assuming you knew the value of the parameters in that model. Suppose that \\(X\\) and \\(Y\\) represent independent flips of a coin (0 = tails, 1 = heads) with probability \\(0.7\\) of landing heads. If you flip a coin two times, what’s the probabiity of observing a tail and then a head (\\(X = 0, Y = 1\\))? In Mathematical Statistics, we will similarly write down a probability model but then we will use observed data to estimate the value of the parameters in that model. Suppose we flipped a coin two times and observed one tail and then one head. Would you be more willing to believe that the probability of getting a head (\\(p\\)) is 0.7 or 0.4? If \\(p\\) could be anywhere between 0 and 1, what’s your best guess about what that value of \\(p\\) is based on the observed data? There is more than one technique that you can use to estimate the value of an unknown parameter. You’re already familiar with one technique—least squares estimation—from STAT 155. We’ll review the ideas behind that approach, as well as explore two other widely used estimation techniques: maximum likelihood estimation and the method of moments. 2.1 Learning Goals Understand the distinction between common parameter estimation methods (e.g., least squares, maximum likelihood, method of moments) Be able to implement common parameter estimation methods in a variety of settings and models 2.2 Least Squares Estimation 2.2.1 (Optional) Textbook Reading Guide Read: Sections 11.1 and 11.2, through the definition of residuals (pages 520–523), and Section 3.4 in the Stat 155 Notes Definitions: method of least squares residual predicted/fitted value Questions: What is the intuition behind least squares estimation? What are the typical steps to find a least squares estimate? How can we use least squares estimation to estimate the slope and intercept of a simple linear regression model? (work through Theorem 11.2.1 in detail and fill in any steps that the textbook left out) 2.2.2 In-Class Notes and Activities See Day 1 Slides and Activity 1 on Moodle. 2.3 Maximum Likelihood Estimation 2.3.1 Textbook Reading Guide Read: Section 5.1 and the first half of Section 5.2 (pages 278–288) Definitions: parameter statistic/estimator estimate likelihood function – Definition 5.2.1 maximum likelihood estimate – Definition 5.2.2 log-likelihood Note: wherever applicable, you should write down definitions for important vocab both “in words” and in mathematical notation. Questions: What is the intuition behind the maximum likelihood estimation (MLE) approach? What are the typical steps to find a MLE? (see Ex 5.2.1, 5.2.2, and Case Study 5.2.1; work through at least one of these examples in detail, filling in any steps that the textbook left out) Are there ever situations when the typical steps to finding a MLE don’t work? If so, what can we do instead to find the MLE? (see Ex 5.2.3, 5.2.4) How do the steps to finding a MLE change when we have more than one unknown parameter? (see Ex 5.2.5) 2.3.2 Videos MLE Intro (Day 2) MLE Example (Day 2) MLE Numerical Optimization (Day 3) 2.3.3 In-Class Notes and Activities MLE “by hand”: see Day 2 Slides and Activity 2 on Moodle. MLE by numerical techniques: see Day 3 Slides and Activity 3 on Moodle. 2.4 The Method of Moments 2.4.1 Textbook Reading Guide Read: the second half of Section 5.2 (pages 289–293) Definitions: theoretical moment sample moment method of moments estimates – Definition 5.2.3 Remember: wherever applicable, you should write down definitions for important vocab both “in words” and in mathematical notation. Questions: What is the intuition behind the method of moments (MOM) procedure for estimating unknown parameters? What are the typical steps to find a MOM estimator? (see Ex 5.2.6, 5.2.7, and Case Study 5.2.2; work through at least one of these examples in detail, filling in any steps that the textbook left out) What advantages does the MOM approach offer compared to MLE? Do the MOM and MLE approaches always yield the same estimate? (look through the examples in Section 5.2 and try using the other approach — do you always get the same answer?) 2.4.2 Videos MOM Intro MOM Example 2.4.3 In-Class Notes and Activities See Day 4 Slides and Activity 4 on Moodle. "],["properties-of-estimators.html", "3 Properties of Estimators 3.1 Learning Goals 3.2 Bias and Variance 3.3 Cramer-Rao Lower Bound 3.4 Consistency and Other Asymptotic Properties", " 3 Properties of Estimators As we’ve seen, there is more than one way to estimate an unknown parameter. Sometimes these different estimation techniques lead to the same estimator, but sometimes they do not. In that case, how can we choose which estimator to use? What makes one estimator “better” than another? 3.1 Learning Goals Evaluate properties of different estimators (e.g., bias, efficiency, mean squared error, consistency) Understand the distinction between finite sample and asymptotic properties Use these properties to choose one type of estimator of another 3.2 Bias and Variance 3.2.1 Textbook Reading Guide Read: Section 5.4 (pages 308–316) Definitions: unbiased asymptotically unbiased more efficient relative efficiency Questions: What is the difference between an estimator and an estimate? What are the properties of a “good” estimator? Intuitively, what is the difference between bias and precision? What are the typical steps to checking if an estimator is unbiased? (see Ex 5.4.2, 5.4.3, and 5.4.4) How can we construct an estimator that is unbiased? (see the Comment in Ex 5.4.2 and 5.4.4) If an estimator is unbiased, is it also asymptotically unbiased? If an estimator is asymptotically unbiased, is it necessarily unbiased? If we are comparing two estimators, how can we check which estimator is more efficient? (see Ex 5.4.5 and 5.4.6) 3.2.2 Videos Bias and Variance Intro Bias Examples Variance Examples 3.2.3 In-Class Notes and Activities See Day 5 Slides and Activity 5 on Moodle. 3.3 Cramer-Rao Lower Bound 3.3.1 Textbook Reading Guide Read: Section 5.5 (pages 316–319) Definitions and Theorems: Cramer-Rao Inequality best or minimum-variance unbiased estimator efficient estimator efficiency of an unbiased estimator Questions: Can you describe, in your own words, what the Cramer-Rao Inequality tells us? What are the typical steps to deriving the Cramer-Rao lower bound (see Ex 5.5.1 and 5.5.2) What is the difference between a best and efficient estimator? Does one imply the other? (see the Comment below Definition 5.5.2) What steps do we need to take to show an estimator is efficient? What assumptions do we need to make in order for the Cramer-Rao Inequality to hold? Can you think of any examples of probability distributions that do not meet these assumptions? (see Ex 5.5.2) 3.3.2 Videos CRLB Intro CRLB Example 3.3.3 In-Class Notes and Activities See Day 6 Slides and Activity 6 on Moodle. 3.4 Consistency and Other Asymptotic Properties 3.4.1 Textbook Reading Guide Read: Section 5.7 (pages 326–329) Definitions and Theorems: asymptotically unbiased consistent Chebyshev’s inequality weak law of large numbers Questions: What is the distinction between a fixed sample property and an asymptotic property of an estimator? Can you describe, in your own words, what it means for an estimator to be consistent? How can we use the \\(\\epsilon-\\delta\\) definition of consistency to show that an estimator is consistent? (see Ex 5.7.1) How can we use Chebyshev’s inequality to show that an estimator is consistent? (see Ex 5.7.2) Which of the estimation techniques that we’ve seen in this class yields consistent estimators? (see Comment after Ex 5.7.2) 3.4.2 Videos Consistency Intro Consistency Examples 3.4.3 In-Class Notes and Activities See Day 7 Slides and Activity 7 on Moodle. "],["bayesian-statistics.html", "4 Bayesian Statistics 4.1 Learning Goals 4.2 Prior and Posterior Distributions 4.3 Bayes Estimators", " 4 Bayesian Statistics Bayesian estimation is an approach for incorporating prior knowledge into the estimation of unknown parameters. The Bayesian philosophy represents a total shift from the Frequentist philosophy that we’ve seen so far in this class, and that you’ve seen in your other stats classes. An important distinction between the two philosophies lies in their beliefs about unknown parameters. In particular: Frequentists believe that parameters are unknown but fixed constants Bayesians believe that parameters are random variables (so they have a probability distribution) 4.1 Learning Goals Understand the difference between the Frequentist and Bayesian philosophies Derive the posterior distribution for an unknown parameter based on a specified prior and likelihood Derive the Bayes estimator for a given loss function Evaluate the properties of Bayes estimators Understand the impact of the choice of prior on Bayesian estimation 4.2 Prior and Posterior Distributions 4.2.1 Textbook Reading Guide Read: first half of Section 5.8 (pages 329–337) Definitions and Theorems: Bayes’ Theorem prior distribution posterior distribution noninformative prior conjugate prior marginal pdf Questions: What is the difference between the Bayesian and Frequentist philosophies? What are the typical steps to deriving a posterior distribution? How is the posterior distribution impacted by the observed data and our choice of prior? What sorts of considerations should we keep in mind in choosing a prior? 4.2.2 Videos Bayesian vs Frequentist Philosophy Prior and Posterior Distributions Intro Prior and Posterior Distributions Example 4.2.3 In-Class Notes and Activities See Day 9 Slides and Activity 9 on Moodle. 4.3 Bayes Estimators 4.3.1 Textbook Reading Guide Read: second half of Section 5.8 (pages 337–341) Definitions: posterior mode posterior median posterior mean loss function risk Bayes estimate Questions: Once we have a posterior distribution for \\(\\theta\\), how can we provide an estimate for \\(\\theta\\)? What are some examples of commonly-used loss functions? What are the typical steps to finding a Bayes estimate? What are the Bayes estimates for absolute error loss and squared error loss? How are the Bayes and maximum likelihood estimators typically related? What criteria do we consider in choosing an “optimal” estimator under the Bayesian paradigm? How do Bayes estimators stand up against Frequentist optimality criteria (e.g., bias, asymptotic bias, consistency)? 4.3.2 Videos Bayesian Estimation Intro Bayesian Estimation Proofs Properties of Bayes Estimators 4.3.3 In-Class Notes and Activities See Day 10 Slides and Activity 10 on Moodle. "],["confidence-intervals.html", "5 Confidence Intervals 5.1 Learning Goals 5.2 Textbook Reading Guide", " 5 Confidence Intervals Under Construction 5.1 Learning Goals Understand the mathematical foundation of, be able to derive, be able to use, and be able to appropriately interpret confidence intervals 5.2 Textbook Reading Guide "],["hypothesis-testing.html", "6 Hypothesis Testing 6.1 Learning Goals 6.2 Textbook Reading Guide", " 6 Hypothesis Testing Under Construction 6.1 Learning Goals Understand the mathematical foundation of, be able to derive, and be able to use common hypothesis tests 6.2 Textbook Reading Guide "]]
